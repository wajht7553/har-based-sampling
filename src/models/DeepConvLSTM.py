##################################################
# Pytorch implementation of the DeepConvLSTM as proposed by by Ordonez and Roggen (https://www.mdpi.com/1424-8220/16/1/115)
# and DeepConvContext with LSTM/self-attention/transformer variants
##################################################

from torch import nn
import torch
import math
import warnings

warnings.filterwarnings("ignore")


class PositionalEncoding(nn.Module):
    """
    Positional encoding for transformer models.
    """

    def __init__(self, d_model, max_len=5000):
        super(PositionalEncoding, self).__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(
            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)
        )
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(1)
        self.register_buffer("pe", pe)

    def forward(self, x):
        x = x + self.pe[: x.size(0), :]
        return x


class DeepConvLSTM(nn.Module):
    """
    DeepConvLSTM model as described in "Deep Convolutional and LSTM Recurrent Neural Networks for Multimodal Wearable Activity Recognition" (https://doi.org/10.1145/3460421.3480419).

    Args:
        channels: int
            Number of channels in the input data.
        classes: int
            Number of classes for classification.
        window_size: int
            Size of the input window.
        conv_kernels: int
            Number of convolutional kernels.
        conv_kernel_size: int
            Size of the convolutional kernels.
        lstm_units: int
            Number of LSTM units.
        lstm_layers: int
            Number of LSTM layers.
        dropout: float
            Dropout rate.
    """

    def __init__(
        self,
        channels,
        classes,
        window_size,
        conv_kernels=64,
        conv_kernel_size=5,
        lstm_units=128,
        lstm_layers=2,
        dropout=0.5,
    ):
        super(DeepConvLSTM, self).__init__()

        self.conv1 = nn.Conv2d(1, conv_kernels, (conv_kernel_size, 1))
        self.conv2 = nn.Conv2d(conv_kernels, conv_kernels, (conv_kernel_size, 1))
        self.conv3 = nn.Conv2d(conv_kernels, conv_kernels, (conv_kernel_size, 1))
        self.conv4 = nn.Conv2d(conv_kernels, conv_kernels, (conv_kernel_size, 1))
        self.lstm = nn.LSTM(channels * conv_kernels, lstm_units, num_layers=lstm_layers)
        self.dropout = nn.Dropout(dropout)
        self.classifier = nn.Linear(lstm_units, classes)
        self.activation = nn.ReLU()
        self.final_seq_len = window_size - (conv_kernel_size - 1) * 4
        self.lstm_units = lstm_units
        self.classes = classes

    def forward(self, x):
        x = x.unsqueeze(1)  # batch, 1, sequence, axes
        x = self.activation(self.conv1(x))  # batch, kernels, sequence, axes
        x = self.activation(self.conv2(x))
        x = self.activation(self.conv3(x))
        x = self.activation(self.conv4(x))
        x = x.permute(2, 0, 3, 1)
        x = x.reshape(x.shape[0], x.shape[1], -1)
        x, _ = self.lstm(x)
        x = x[-1, :, :]
        x = x.view(-1, self.lstm_units)
        x = self.dropout(x)
        return self.classifier(x)

    def number_of_parameters(self):
        return sum(p.numel() for p in self.parameters() if p.requires_grad)


class DeepConvContext(nn.Module):
    """
    DeepConvContext model with LSTM, self-attention, or transformer variants.

    Args:
        batch_size: int
            Size of the input batch.
        channels: int
            Number of channels in the input data.
        classes: int
            Number of classes for classification.
        window_size: int
            Size of the input window.
        conv_kernels: int
            Number of convolutional kernels.
        conv_kernel_size: int
            Size of the convolutional kernels.
        lstm_units: int
            Number of LSTM units.
        lstm_layers: int
            Number of LSTM layers.
        dropout: float
            Dropout rate.
        bidirectional: bool
            Whether to use a bidirectional LSTM.
        type: str
            Type of context model ('lstm', 'self-attention', or 'transformer').
        attention_num_heads: int
            Number of attention heads for self-attention and transformer models.
        transformer_depth: int
            Depth of the transformer model.
    """

    def __init__(
        self,
        batch_size,
        channels,
        classes,
        window_size,
        conv_kernels=64,
        conv_kernel_size=5,
        lstm_units=128,
        lstm_layers=2,
        dropout=0.5,
        bidirectional=False,
        type="lstm",
        attention_num_heads=4,
        transformer_depth=6,
    ):
        super(DeepConvContext, self).__init__()
        self.final_seq_len = window_size - (conv_kernel_size - 1) * 4
        self.conv1 = nn.Conv2d(1, conv_kernels, (conv_kernel_size, 1))
        self.conv2 = nn.Conv2d(conv_kernels, conv_kernels, (conv_kernel_size, 1))
        self.conv3 = nn.Conv2d(conv_kernels, conv_kernels, (conv_kernel_size, 1))
        self.conv4 = nn.Conv2d(conv_kernels, conv_kernels, (conv_kernel_size, 1))
        self.window_lstm = nn.LSTM(
            channels * conv_kernels,
            lstm_units,
            num_layers=lstm_layers,
            batch_first=True,
        )
        self.fc_project = nn.Linear(lstm_units * self.final_seq_len, lstm_units)
        if type == "lstm":
            self.context_lstm = nn.LSTM(
                lstm_units,
                lstm_units,
                num_layers=lstm_layers,
                batch_first=False,
                bidirectional=bidirectional,
            )
        elif type == "self-attention":
            self.is_causal = not bidirectional
            self.context_lstm = torch.nn.MultiheadAttention(
                embed_dim=lstm_units, num_heads=attention_num_heads, batch_first=False
            )
        elif type == "transformer":
            self.is_causal = not bidirectional
            self.context_lstm = torch.nn.Transformer(
                d_model=lstm_units,
                nhead=attention_num_heads,
                num_encoder_layers=transformer_depth,
                num_decoder_layers=transformer_depth,
                dim_feedforward=attention_num_heads * lstm_units,
                batch_first=False,
            )
        self.dropout = nn.Dropout(dropout)
        if bidirectional and type == "lstm":
            self.classifier = nn.Linear(2 * lstm_units, classes)
        else:
            self.classifier = nn.Linear(lstm_units, classes)
        self.bidirectional = bidirectional
        self.type = type
        self.pos_enc = PositionalEncoding(lstm_units, max_len=batch_size)
        self.activation = nn.ReLU()
        self.lstm_units = lstm_units
        self.classes = classes

    def forward(self, x):
        x = x.unsqueeze(1)  # batch, 1, sequence, axes
        x = self.activation(self.conv1(x))  # batch, kernels, sequence, axes
        x = self.activation(self.conv2(x))
        x = self.activation(self.conv3(x))
        x = self.activation(self.conv4(x))
        x = x.permute(0, 2, 3, 1)  # batch, sequence, axes, kernels
        x = x.reshape(x.shape[0], x.shape[1], -1)  # batch, sequence, axes*kernels
        x, _ = self.window_lstm(x)  # batch, sequence, lstm_units
        x = x.reshape(x.shape[0], -1)  # batch, sequence*lstm_units
        x = self.fc_project(x)  # batch, lstm_units
        x = x.unsqueeze(1)  # batch, 1, lstm_units
        if isinstance(self.context_lstm, torch.nn.Transformer):
            x = self.pos_enc(x)
            if self.is_causal:
                seq_len = x.size(0)
                attn_mask = torch.nn.Transformer.generate_square_subsequent_mask(
                    seq_len
                ).to(x.device)
            else:
                attn_mask = None
            x = self.context_lstm(
                x,
                x,
                src_is_causal=self.is_causal,
                tgt_is_causal=self.is_causal,
                src_mask=attn_mask,
                tgt_mask=attn_mask,
            )
        elif isinstance(self.context_lstm, torch.nn.MultiheadAttention):
            x = self.pos_enc(x)
            if self.is_causal:
                seq_len = x.size(0)
                attn_mask = torch.nn.Transformer.generate_square_subsequent_mask(
                    seq_len
                ).to(x.device)
            else:
                attn_mask = None
            x, _ = self.context_lstm(x, x, x, attn_mask=attn_mask)
        else:
            x, _ = self.context_lstm(x)
        if self.bidirectional and isinstance(self.context_lstm, torch.nn.LSTM):
            x = x.view(-1, 2 * self.lstm_units)
        else:
            x = x.view(-1, self.lstm_units)
        x = self.dropout(x)
        return self.classifier(x)

    def number_of_parameters(self):
        return sum(p.numel() for p in self.parameters() if p.requires_grad)
